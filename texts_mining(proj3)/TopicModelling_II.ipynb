{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "txzt3HoPoRVj",
    "outputId": "2e893aba-7f97-4c38-d84d-012448bac963"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ku-team\\Anaconda3\\lib\\site-packages\\past\\builtins\\misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "# nltk.download('all')   # no need to download all packages, uncomment it for 1st time running code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re    # regular expression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint  # prettier print\n",
    "import gensim       # Gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "IrDaKTdQobkx",
    "outputId": "63d8bd0b-ec3e-4d7c-ddbc-1748c03116cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our stay with Marcus in Bristol was fantastic in every way! He was a great '\n",
      " 'host - picking us up at the bus stop, recommending places to try, leaving '\n",
      " 'plenty of pastries and other breakfast items to enjoy in the morning. The '\n",
      " 'flat itself was modern, bright, clean and spacious - and best of all, right '\n",
      " \"on Bristol's lovely harbourside. We will definitely stay again next time \"\n",
      " \"we're in Bristol - thanks again Marcus!\"]\n"
     ]
    }
   ],
   "source": [
    "# load the save the english comments corpus into a pickle file as a list\n",
    "pickle_in = open(\"english_comments.pickle\",\"rb\")\n",
    "english_comments = pickle.load(pickle_in)    # loads the list of comments\n",
    "pprint(english_comments[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "-t8Drr64oveV",
    "outputId": "d8c8d902-eb92-41cf-ef7d-7ec1f162f42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            comments\n",
      "0  Our stay with Marcus in Bristol was fantastic ...\n",
      "1  Marcus is a brilliant, warm and friendly host....\n",
      "2  My mum Angela and I have stayed at Marcus' ama...\n",
      "3  Marcus was an exceptional host. I only stayed ...\n",
      "4  Marcus was welcoming, easy going and very help...\n",
      "\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 96419 entries, 0 to 96418\n",
      "Data columns (total 1 columns):\n",
      "comments    96419 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 753.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# put the list into dataframe to view the nature of texts \n",
    "df = pd.DataFrame(english_comments,columns=['comments'])\n",
    "pprint(df.head(5))\n",
    "print('\\n\\n')\n",
    "pprint(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbrydJKNo_aj"
   },
   "outputs": [],
   "source": [
    "# tokenize sentences and remove panctuations and emojis\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "yU59hE6hpM8n",
    "outputId": "941e48ff-b57e-481e-a7bc-6419c9ccb1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['our', 'stay', 'with', 'marcus', 'in', 'bristol', 'was', 'fantastic', 'in', 'every', 'way', 'he', 'was', 'great', 'host', 'picking', 'us', 'up', 'at', 'the', 'bus', 'stop', 'recommending', 'places', 'to', 'try', 'leaving', 'plenty', 'of', 'pastries', 'and', 'other', 'breakfast', 'items', 'to', 'enjoy', 'in', 'the', 'morning', 'the', 'flat', 'itself', 'was', 'modern', 'bright', 'clean', 'and', 'spacious', 'and', 'best', 'of', 'all', 'right', 'on', 'bristol', 'lovely', 'harbourside', 'we', 'will', 'definitely', 'stay', 'again', 'next', 'time', 'we', 're', 'in', 'bristol', 'thanks', 'again', 'marcus'], ['marcus', 'is', 'brilliant', 'warm', 'and', 'friendly', 'host', 'he', 'picked', 'us', 'up', 'from', 'the', 'railway', 'station', 'he', 'took', 'anne', 'to', 'the', 'doctor', 'and', 'drove', 'us', 'around', 'wherever', 'we', 'needed', 'to', 'go', 'in', 'bristol', 'and', 'dropped', 'us', 'back', 'at', 'the', 'railway', 'station', 'when', 'we', 'were', 'leaving', 'his', 'flat', 'is', 'very', 'modern', 'comfortable', 'and', 'clean', 'and', 'is', 'very', 'well', 'heated', 'marcus', 'provided', 'us', 'with', 'everything', 'we', 'could', 'wish', 'for', 'we', 'wish', 'we', 'could', 'have', 'stayed', 'longer']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = list(sent_to_words(english_comments))\n",
    "print(tokenized_texts[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1_KH5BAdq0Da",
    "outputId": "61a4575b-4517-49db-99fe-9a6f2aae9fbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('and', 186280),\n",
       " ('bristol', 26587),\n",
       " ('clean', 25628),\n",
       " ('for', 47042),\n",
       " ('great', 49954),\n",
       " ('in', 64651),\n",
       " ('is', 63608),\n",
       " ('it', 32402),\n",
       " ('location', 30508),\n",
       " ('lovely', 30187),\n",
       " ('of', 41167),\n",
       " ('place', 32685),\n",
       " ('stay', 43553),\n",
       " ('the', 154077),\n",
       " ('to', 93356),\n",
       " ('very', 59606),\n",
       " ('was', 73952),\n",
       " ('we', 42815),\n",
       " ('with', 37414),\n",
       " ('you', 26730)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the most common and most rare words\n",
    "from collections import  Counter    # dictionary collection\n",
    "def get_words_frequency(texts):\n",
    "    counter = Counter()\n",
    "    for  doc in texts:\n",
    "        for word in doc:\n",
    "            counter[word]+=1\n",
    "    return counter\n",
    "\n",
    "word_freq = get_words_frequency(tokenized_texts)   \n",
    "#pprint(word_freq) # uncomment this line to see that @ the end of the printed texts, still chinise words in there.\n",
    "first10pairs = {k for k in sorted(word_freq.items(),key=lambda x: x[1],reverse=True)[:20]}\n",
    "first10pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NDOHHLOZAZ9v",
    "outputId": "6820e266-3157-4a58-aa1b-249809b6f85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['our', 'stay', 'with', 'marcus', 'in', 'bristol', 'was', 'fantastic', 'in', 'every', 'way', 'he', 'was', 'great', 'host', 'picking', 'us', 'up', 'at', 'the', 'bus', 'stop', 'recommending', 'places', 'to', 'try', 'leaving', 'plenty', 'of', 'pastries', 'and', 'other', 'breakfast', 'items', 'to', 'enjoy', 'in', 'the', 'morning', 'the', 'flat', 'itself', 'was', 'modern', 'bright', 'clean', 'and', 'spacious', 'and', 'best', 'of', 'all', 'right', 'on', 'bristol', 'lovely', 'harbourside', 'we', 'will', 'definitely', 'stay', 'again', 'next', 'time', 'we', 're', 'in', 'bristol', 'thanks', 'again', 'marcus'], ['marcus', 'is', 'brilliant', 'warm', 'and', 'friendly', 'host', 'he', 'picked', 'us', 'up', 'from', 'the', 'railway', 'station', 'he', 'took', 'anne', 'to', 'the', 'doctor', 'and', 'drove', 'us', 'around', 'wherever', 'we', 'needed', 'to', 'go', 'in', 'bristol', 'and', 'dropped', 'us', 'back', 'at', 'the', 'railway', 'station', 'when', 'we', 'were', 'leaving', 'his', 'flat', 'is', 'very', 'modern', 'comfortable', 'and', 'clean', 'and', 'is', 'very', 'well', 'heated', 'marcus', 'provided', 'us', 'with', 'everything', 'we', 'could', 'wish', 'for', 'we', 'wish', 'we', 'could', 'have', 'stayed', 'longer']]\n"
     ]
    }
   ],
   "source": [
    "# to filter chinese words or get only english words\n",
    "def get_english_words(documents):\n",
    "    ''' filters out any non-english words'''\n",
    "    english_words = [[ w for w in doc if re.match(r'[A-Z, a-z]', w)] for doc in documents]\n",
    "    return english_words\n",
    "\n",
    "\n",
    "fitered_english_words = get_english_words(tokenized_texts)\n",
    "print(fitered_english_words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZTlG5-HFMXKY",
    "outputId": "e6d410a2-0cbe-4ce7-e6ef-a4c9eaa89c1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 4,\n",
       " 'aaalso': 1,\n",
       " 'aagain': 1,\n",
       " 'aardman': 1,\n",
       " 'aardvark': 1,\n",
       " 'aaron': 3,\n",
       " 'aasy': 1,\n",
       " 'ab': 4,\n",
       " 'aback': 2,\n",
       " 'abandoned': 5,\n",
       " 'abase': 1,\n",
       " 'abb': 3,\n",
       " 'abbey': 11,\n",
       " 'abbeywood': 1,\n",
       " 'abbi': 12,\n",
       " 'abbiamo': 1,\n",
       " 'abbie': 3,\n",
       " 'abbots': 1,\n",
       " 'abby': 2,\n",
       " 'abc': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see words and their frequency\n",
    "words_freq=  get_words_frequency(fitered_english_words)\n",
    "first10pairs = {k: words_freq[k] for k in sorted(words_freq.keys())[:20]}\n",
    "first10pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JFiZxYwdO3mq",
    "outputId": "88a6173d-0333-4b94-c56d-6d0f51660dd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 186280),\n",
       " ('the', 154077),\n",
       " ('to', 93356),\n",
       " ('was', 73952),\n",
       " ('in', 64651),\n",
       " ('is', 63608),\n",
       " ('very', 59606),\n",
       " ('great', 49954),\n",
       " ('for', 47042),\n",
       " ('stay', 43553),\n",
       " ('we', 42815),\n",
       " ('of', 41167),\n",
       " ('with', 37414),\n",
       " ('place', 32685),\n",
       " ('it', 32402),\n",
       " ('location', 30508),\n",
       " ('lovely', 30187),\n",
       " ('you', 26730),\n",
       " ('bristol', 26587),\n",
       " ('clean', 25628)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the most common or frequent words\n",
    "most_common_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "most_common_words[:20]   # first 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7mpBu6UCV6At",
    "outputId": "68791ff8-51f4-4716-e426-2c3e20fe91be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chauffeur', 1),\n",
      " ('ireland', 1),\n",
      " ('foreward', 1),\n",
      " ('boatload', 1),\n",
      " ('markus', 1),\n",
      " ('cellular', 1),\n",
      " ('hovered', 1),\n",
      " ('saunter', 1),\n",
      " ('arranges', 1),\n",
      " ('availablity', 1)]\n"
     ]
    }
   ],
   "source": [
    "# see the rare words \n",
    "rare_words = sorted(word_freq.items(), key=lambda x: x[1])   # NB: word_freq is a dictionary colloction\n",
    "pprint(rare_words[:10])  #  top 10 rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "1_F6pXbYXjve",
    "outputId": "aad8a399-f6da-43e5-b2d8-b2a0c4ec479d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'stay', 'with', 'marcus', 'in', 'bristol', 'was', 'fantastic', 'in', 'every', 'way', 'he', 'was', 'great', 'host', 'picking', 'us', 'up', 'at', 'the', 'bus', 'stop', 'recommending', 'places', 'to', 'try', 'leaving', 'plenty', 'of', 'pastries', 'and', 'other', 'breakfast', 'items', 'to', 'enjoy', 'in', 'the', 'morning', 'the', 'flat', 'itself', 'was', 'modern', 'bright', 'clean', 'and', 'spacious', 'and', 'best', 'of', 'all', 'right', 'on', 'bristol', 'lovely', 'harbourside', 'we', 'will', 'definitely', 'stay', 'again', 'next', 'time', 'we', 're', 'in', 'bristol', 'thanks', 'again', 'marcus']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram & trigram models  [the model creates 2 or 3 wordy, most common occuring  phares]\n",
    "bigram = gensim.models.Phrases(fitered_english_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[fitered_english_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "print(trigram_mod[bigram_mod[fitered_english_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WTxxgiuYYHPM",
    "outputId": "b13fcb23-0833-46aa-a06b-69e733250fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stop words: 179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "print('number of stop words:',len(stop_words))\n",
    "\n",
    "# define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatize_texts(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data_out =[]\n",
    "    for doc in texts:\n",
    "            data_out.append([lemmatizer.lemmatize(word) for word in doc])\n",
    "    return data_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "ihYJn6pEYKAy",
    "outputId": "917f5b33-71bf-4851-930d-bfcb3e01dcdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing stop words: ['stay', 'marcus', 'bristol', 'fantastic', 'every', 'way', 'great', 'host', 'picking', 'us', 'bus', 'stop', 'recommending', 'places', 'try', 'leaving', 'plenty', 'pastries', 'breakfast', 'items', 'enjoy', 'morning', 'flat', 'modern', 'bright', 'clean', 'spacious', 'best', 'right', 'bristol', 'lovely', 'harbourside', 'definitely', 'stay', 'next', 'time', 'bristol', 'thanks', 'marcus']\n",
      "\n",
      "bigram data words: ['stay', 'marcus', 'bristol', 'fantastic', 'every', 'way', 'great', 'host', 'picking', 'us', 'bus', 'stop', 'recommending', 'places', 'try', 'leaving', 'plenty', 'pastries', 'breakfast', 'items', 'enjoy', 'morning', 'flat', 'modern', 'bright', 'clean', 'spacious', 'best', 'right', 'bristol', 'lovely', 'harbourside', 'definitely', 'stay', 'next', 'time', 'bristol', 'thanks', 'marcus']\n",
      "\n",
      "\n",
      "After lemmatization: [['stay', 'marcus', 'bristol', 'fantastic', 'every', 'way', 'great', 'host', 'picking', 'u', 'bus', 'stop', 'recommending', 'place', 'try', 'leaving', 'plenty', 'pastry', 'breakfast', 'item', 'enjoy', 'morning', 'flat', 'modern', 'bright', 'clean', 'spacious', 'best', 'right', 'bristol', 'lovely', 'harbourside', 'definitely', 'stay', 'next', 'time', 'bristol', 'thanks', 'marcus']]\n"
     ]
    }
   ],
   "source": [
    "# remove the stopwords\n",
    "data_words_nostops = remove_stopwords(fitered_english_words)\n",
    "print(\"after removing stop words:\",data_words_nostops[0])\n",
    "\n",
    "# create bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "print(\"\\nbigram data words:\",data_words_bigrams[0])\n",
    "\n",
    "#  lemmatize nouns\n",
    "lemmatized_data = lemmatize_texts(data_words_bigrams)\n",
    "print('\\n\\nAfter lemmatization:',lemmatized_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "41nuT4RodQLV",
    "outputId": "3e5549f3-3c76-4d19-9497-da85ccff4db1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 49961),\n",
       " ('stay', 43840),\n",
       " ('place', 35320),\n",
       " ('location', 30625),\n",
       " ('lovely', 30188),\n",
       " ('host', 28772),\n",
       " ('bristol', 26618),\n",
       " ('clean', 25632),\n",
       " ('room', 23979),\n",
       " ('would', 21626),\n",
       " ('really', 19666),\n",
       " ('house', 19255),\n",
       " ('nice', 18826),\n",
       " ('comfortable', 18213),\n",
       " ('recommend', 18204),\n",
       " ('good', 16698),\n",
       " ('flat', 16037),\n",
       " ('u', 13653),\n",
       " ('home', 13089),\n",
       " ('everything', 12855)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after removing stop words or pre-processing : check the most common and very the rare words\n",
    "wf = get_words_frequency(lemmatized_data)\n",
    "most_common_words = sorted(wf.items(), key=lambda x: x[1], reverse=True)\n",
    "most_common_words[:20]   # top 20 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iS3EpGgifMBr",
    "outputId": "d3012f59-363f-4af0-83a4-5127b0d889fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chauffeur', 1),\n",
      " ('ireland', 1),\n",
      " ('foreward', 1),\n",
      " ('markus', 1),\n",
      " ('cellular', 1),\n",
      " ('hovered', 1),\n",
      " ('saunter', 1),\n",
      " ('arranges', 1),\n",
      " ('availablity', 1),\n",
      " ('macus', 1),\n",
      " ('beet', 1),\n",
      " ('fraternity', 1),\n",
      " ('spotlesly', 1),\n",
      " ('guast', 1),\n",
      " ('vegfest', 1),\n",
      " ('deny', 1),\n",
      " ('wecolming', 1),\n",
      " ('strives', 1),\n",
      " ('morethe', 1),\n",
      " ('lovliness', 1)]\n"
     ]
    }
   ],
   "source": [
    "# after removing stop words or pre-processing : check the rare words\n",
    "wf = get_words_frequency(lemmatized_data)\n",
    "rare_words = sorted(wf.items(), key=lambda x: x[1])\n",
    "pprint(rare_words[:20])   # first 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GcfT_QNusnWy"
   },
   "outputs": [],
   "source": [
    "def remove_rare_words(texts,words_freq):\n",
    "    '''removes the rare words'''\n",
    "    words_data = [[w for w in doc if words_freq[w]>7] for doc in texts]\n",
    "    return words_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3Kbv4DQjoexy",
    "outputId": "6c732b29-a97f-4f94-f968-c3ccc9cb8bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stay', 'marcus', 'bristol', 'fantastic', 'every', 'way', 'great', 'host', 'picking', 'u', 'bus', 'stop', 'recommending', 'place', 'try', 'leaving', 'plenty', 'pastry', 'breakfast', 'item', 'enjoy', 'morning', 'flat', 'modern', 'bright', 'clean', 'spacious', 'best', 'right', 'bristol', 'lovely', 'harbourside', 'definitely', 'stay', 'next', 'time', 'bristol', 'thanks', 'marcus'], ['marcus', 'brilliant', 'warm', 'friendly', 'host', 'picked', 'u', 'railway_station', 'took', 'anne', 'doctor', 'drove', 'u', 'around', 'wherever', 'needed', 'go', 'bristol', 'dropped', 'u', 'back', 'railway_station', 'leaving', 'flat', 'modern', 'comfortable', 'clean', 'well', 'heated', 'marcus', 'provided', 'u', 'everything', 'could', 'wish', 'wish', 'could', 'stayed', 'longer'], ['mum', 'angela', 'stayed', 'marcus', 'amazing', 'apartment', 'two', 'week', 'august', 'relocating', 'bristol', 'lovely', 'experience', 'host', 'apartment', 'extremely', 'confortable', 'located', 'nice', 'area', 'bristol', 'close', 'shop', 'several', 'amenity', 'marcus', 'great', 'person', 'provided', 'u', 'needed', 'plus', 'generous', 'tip', 'practical', 'help', 'made', 'relocation', 'much', 'easier', 'also', 'fun', 'recommend', 'company', 'place', 'two', 'special', 'ingredient', 'visit', 'bristol', 'surrounding_areas'], ['marcus', 'exceptional', 'host', 'stayed', 'one', 'night', 'nicely', 'offered', 'around', 'bristol', 'raining', 'life', 'flat', 'location', 'harbourside', 'great', 'stay'], ['marcus', 'welcoming', 'easy', 'going', 'helpful', 'informative', 'place', 'gorgeous', 'comfy', 'perfectly', 'located', 'walking', 'everything', 'great', 'restaurant', 'shop', 'many', 'place', 'interest', 'stay', 'wholeheartedly', 'recommend']]\n"
     ]
    }
   ],
   "source": [
    "# remove the rare words from texts\n",
    "words_data = remove_rare_words(lemmatized_data,wf)\n",
    "print(words_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "colab_type": "code",
    "id": "qkAmuR7bpv9R",
    "outputId": "341ad157-22dc-4213-e705-e2507f167bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of keys: 5785\n",
      "best\n",
      "[['stay', 'marcus', 'bristol', 'fantastic', 'every', 'way', 'great', 'host', 'picking', 'u', 'bus', 'stop', 'recommending', 'place', 'try', 'leaving', 'plenty', 'pastry', 'breakfast', 'item', 'enjoy', 'morning', 'flat', 'modern', 'bright', 'clean', 'spacious', 'best', 'right', 'bristol', 'lovely', 'harbourside', 'definitely', 'stay', 'next', 'time', 'bristol', 'thanks', 'marcus'], ['marcus', 'brilliant', 'warm', 'friendly', 'host', 'picked', 'u', 'railway_station', 'took', 'anne', 'doctor', 'drove', 'u', 'around', 'wherever', 'needed', 'go', 'bristol', 'dropped', 'u', 'back', 'railway_station', 'leaving', 'flat', 'modern', 'comfortable', 'clean', 'well', 'heated', 'marcus', 'provided', 'u', 'everything', 'could', 'wish', 'wish', 'could', 'stayed', 'longer'], ['mum', 'angela', 'stayed', 'marcus', 'amazing', 'apartment', 'two', 'week', 'august', 'relocating', 'bristol', 'lovely', 'experience', 'host', 'apartment', 'extremely', 'confortable', 'located', 'nice', 'area', 'bristol', 'close', 'shop', 'several', 'amenity', 'marcus', 'great', 'person', 'provided', 'u', 'needed', 'plus', 'generous', 'tip', 'practical', 'help', 'made', 'relocation', 'much', 'easier', 'also', 'fun', 'recommend', 'company', 'place', 'two', 'special', 'ingredient', 'visit', 'bristol', 'surrounding_areas']]\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)], [(3, 1), (5, 1), (10, 1), (13, 1), (15, 1), (17, 2), (18, 1), (33, 4), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 2)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('best', 1),\n",
       "  ('breakfast', 1),\n",
       "  ('bright', 1),\n",
       "  ('bristol', 3),\n",
       "  ('bus', 1),\n",
       "  ('clean', 1),\n",
       "  ('definitely', 1),\n",
       "  ('enjoy', 1),\n",
       "  ('every', 1),\n",
       "  ('fantastic', 1),\n",
       "  ('flat', 1),\n",
       "  ('great', 1),\n",
       "  ('harbourside', 1),\n",
       "  ('host', 1),\n",
       "  ('item', 1),\n",
       "  ('leaving', 1),\n",
       "  ('lovely', 1),\n",
       "  ('marcus', 2),\n",
       "  ('modern', 1),\n",
       "  ('morning', 1),\n",
       "  ('next', 1),\n",
       "  ('pastry', 1),\n",
       "  ('picking', 1),\n",
       "  ('place', 1),\n",
       "  ('plenty', 1),\n",
       "  ('recommending', 1),\n",
       "  ('right', 1),\n",
       "  ('spacious', 1),\n",
       "  ('stay', 2),\n",
       "  ('stop', 1),\n",
       "  ('thanks', 1),\n",
       "  ('time', 1),\n",
       "  ('try', 1),\n",
       "  ('u', 1),\n",
       "  ('way', 1)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dictionary\n",
    "id2word = corpora.Dictionary(words_data)\n",
    "print(\"number of keys:\",len(id2word.keys()))\n",
    "# view the first word of the dictionary\n",
    "print(id2word[0])   # see word with key =0\n",
    "\n",
    "# create corpus\n",
    "corpus_texts= words_data\n",
    "print(corpus_texts[:3])\n",
    "\n",
    "# bag of words or vector spaces\n",
    "corpus = [id2word.doc2bow(text) for text in corpus_texts]\n",
    "print(corpus[:2])\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GvRkH2GzqYkS"
   },
   "source": [
    "How does LDA work or converge?\n",
    "`\n",
    "For each document d, compute P( topic t | document d ) := proportion of words in document d that are assigned to topic t\n",
    "For each topic t, P( word w | topic t ) := proportion of assignments to topic t that come from word w (across all documents)\n",
    "For each word w, reassign topic t’, where we choose topic t’ with probability P( topic t’ | word w ) = P( topic t’ | document d ) * P( word w | topic t’ )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "28bLfPheqNEg",
    "outputId": "f3e5183d-edb6-4175-9d3d-7ce4b9cc5357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' 512.442 secs'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "done_time = time.time()\n",
    "\n",
    "pprint(\" %.3f secs\" % (done_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "colab_type": "code",
    "id": "LeWlD5B1yGyh",
    "outputId": "221e89cc-0eb8-4c30-e29d-9accf071d240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.249*\"nice\" + 0.219*\"good\" + 0.062*\"value\" + 0.043*\"shower\" + '\n",
      "  '0.040*\"people\" + 0.036*\"money\" + 0.012*\"window\" + 0.010*\"service\" + '\n",
      "  '0.010*\"drink\" + 0.010*\"load\"'),\n",
      " (1,\n",
      "  '0.077*\"bit\" + 0.044*\"milk\" + 0.040*\"booking\" + 0.030*\"bread\" + '\n",
      "  '0.030*\"heating\" + 0.025*\"another\" + 0.023*\"fridge\" + 0.022*\"outstanding\" + '\n",
      "  '0.021*\"inside\" + 0.020*\"others\"'),\n",
      " (2,\n",
      "  '0.104*\"clean\" + 0.094*\"lovely\" + 0.072*\"really\" + 0.060*\"room\" + '\n",
      "  '0.058*\"house\" + 0.056*\"comfortable\" + 0.037*\"bed\" + 0.034*\"home\" + '\n",
      "  '0.030*\"friendly\" + 0.026*\"communication\"'),\n",
      " (3,\n",
      "  '0.089*\"coffee\" + 0.071*\"couple\" + 0.063*\"tea\" + 0.039*\"especially\" + '\n",
      "  '0.033*\"group\" + 0.033*\"sofa\" + 0.024*\"decor\" + 0.024*\"wanted\" + '\n",
      "  '0.023*\"basic\" + 0.023*\"slept\"'),\n",
      " (4,\n",
      "  '0.049*\"well\" + 0.045*\"easy\" + 0.041*\"city\" + 0.040*\"space\" + 0.038*\"centre\" '\n",
      "  '+ 0.032*\"area\" + 0.030*\"close\" + 0.030*\"walk\" + 0.028*\"quiet\" + '\n",
      "  '0.027*\"check\"'),\n",
      " (5,\n",
      "  '0.054*\"u\" + 0.035*\"also\" + 0.032*\"kitchen\" + 0.024*\"could\" + 0.024*\"get\" + '\n",
      "  '0.020*\"local\" + 0.017*\"breakfast\" + 0.017*\"even\" + 0.016*\"around\" + '\n",
      "  '0.016*\"felt\"'),\n",
      " (6,\n",
      "  '0.047*\"night\" + 0.039*\"one\" + 0.039*\"cosy\" + 0.037*\"bathroom\" + '\n",
      "  '0.031*\"stayed\" + 0.029*\"walking\" + 0.025*\"bedroom\" + 0.024*\"airbnb\" + '\n",
      "  '0.023*\"distance\" + 0.022*\"find\"'),\n",
      " (7,\n",
      "  '0.099*\"great\" + 0.087*\"stay\" + 0.078*\"place\" + 0.060*\"location\" + '\n",
      "  '0.056*\"host\" + 0.044*\"bristol\" + 0.043*\"would\" + 0.037*\"recommend\" + '\n",
      "  '0.031*\"flat\" + 0.030*\"apartment\"'),\n",
      " (8,\n",
      "  '0.063*\"time\" + 0.062*\"amazing\" + 0.052*\"touch\" + 0.045*\"back\" + '\n",
      "  '0.039*\"like\" + 0.027*\"make\" + 0.024*\"kind\" + 0.024*\"view\" + 0.023*\"come\" + '\n",
      "  '0.022*\"go\"'),\n",
      " (9,\n",
      "  '0.090*\"little\" + 0.061*\"quick\" + 0.055*\"shop\" + 0.055*\"restaurant\" + '\n",
      "  '0.048*\"clifton\" + 0.042*\"equipped\" + 0.035*\"bar\" + 0.030*\"plenty\" + '\n",
      "  '0.029*\"nearby\" + 0.025*\"cafe\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the 10 keyword in topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "82MTO_SC7Mbc"
   },
   "source": [
    "` NB: next steps (a)further preprocesing texts,like removing the firsrt few most common words,(b)interpretation of the topics,(c)visualizing the topics clusters, and (d) documents ranking...`\n",
    "--------------------\n",
    "`references`: gensim documentation "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TopicModelling_part2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
